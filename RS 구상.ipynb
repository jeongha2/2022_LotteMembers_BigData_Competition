{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86fb3fe3",
   "metadata": {},
   "source": [
    "- 기존 추천 시스템은 구매한 아이템을 기반으로 구매하지 않은 아이템 추천\n",
    "- but, 우리는 구매한 아이템도 추천해야한다.\n",
    "- 그러면 어떻게?\n",
    "  - 기존 NCF, Glocal-K 등 방식은 무리가 있다.\n",
    "    - sequence 정보가 들어가지 않기 때문\n",
    "  - Sequence 정보가 들어가야 할 것 같다.\n",
    "    - 하지만 sequence의 기준은 구매 이력이 아닌 시간으로 해야할 것 같다.\n",
    "    - 적어도 하루 단위 이상으로 해야할 듯!\n",
    "      - 하루는 의미 없을 것 같음(재방문율 한번 분석해보자! 아마 없을 것 같다,,,)\n",
    "      - 또한 이 단위가 너무 작으면 sequence가 너무 길어져,,\n",
    "        - Long Range Dependence!!!\n",
    "        - 차원의 저주 및 계산 시간 증가\n",
    "        - **적절한 window 크기**를 정하면 문제가 적을 것 같다!!!\n",
    "- 하지만 구매 안한 시간이 더 많을테니 sparse해질 듯?\n",
    "  - 이것 해결 방안?\n",
    "  - 구매 안한 부분도 token으로 채워줘야 함: 따로 embedding 구해야겠지?\n",
    "    - 이건 이것만의 의미가 있을 것이다.\n",
    "\n",
    "\n",
    "- ㅁㅊ 아이템 구매 이력이 아닌 시간 간격으로 sequence 모델링한 논문이 있네?\n",
    "  - TiSASRec: Time Interval Aware Self-Attention for Sequential Recommendation\n",
    "  - GRU4Rec+: Session Based Rec\n",
    "  - MARank가 2020 당시 SOTA? : Multi-Order Attentive Ranking Model for Sequential Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ba2fc",
   "metadata": {},
   "source": [
    "- 추천 시스템\n",
    "  - Glocal-K\n",
    "    - AE 사전학습(input: Rating Matrix) 후 fine-tuning\n",
    "      - 저차원으로 축소: 유저-아이템 관계 비선형성으로 학습\n",
    "    - local kernel, global kernel\n",
    "      - local kernel: AE 사전 훈련\n",
    "        - $r'$: AE output, $LK$: RBF kernel(U,V) output\n",
    "        - $W \\dot LK$가 encoder의 가중치로 사용됨\n",
    "      - global kernel은 CNN 구조에서 영감 받음\n",
    "        - $\\mu_i$: $r'_i$를 avgpool, $k_i$: 그냥 가중치\n",
    "        - $GK = \\Sigma \\mu_i \\cdot k_i$: global kernel output\n",
    "        - $GK$로 fine-tuning\n",
    "  - Bert4Rec\n",
    "    - $U$: user vector, $V$: item vector\n",
    "    - $S = {v_1^{(u)}, v_2^{(u)}, \\dots, v_{n_u}^{(u)}}$: 유저가 해당 sequence에 거래한 아이템 v의 집합\n",
    "      - 이 $S$가 모델의 input\n",
    "    - input embedding은 Lookup table 참조\n",
    "    - Positional embedding을 학습\n",
    "    - BERT 구조 사용(Transformer Encoder)\n",
    "      - Bi-directional 효과!! 매우 좋음\n",
    "    - Cloze task(MLM)로 학습(sliding window 안써요\\~~)\n",
    "      - 학습 비용이 크지 않다는 장점(1개의 데이터에 여러 개의 마스킹 있기 때문)\n",
    "      - 근데 학습할 때 그 다음 아이템의 영향도 받는데 이건 괜찮나 모르겠당~\n",
    "        - 만약 이걸로 태클 걸면 학습 과정에서 item의 embedding 값을 학습하게 되는데, 그 다음 아이템의 영향을 받으면 해당 embedding을 사용한 test에서더욱 좋다. 라고 말하면 어떨지?         - 영 꺼림직하면 GPT(Transformer Encoder)을 사용하던지~\n",
    "    - test 시에는 마지막 sequence에 \\[Mask\\] 토큰 추가하여 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4df5fc0",
   "metadata": {},
   "source": [
    "### 실험 설계\n",
    "- 암시적 피드백이 맞는듯\n",
    "  - 우리 데이터에 score가 없음\n",
    "- 평가지표\n",
    "  - HR, NDCG\n",
    "  - K = ??\n",
    "    - 이 K에 따라서 마케팅 전략 수행하면 좋을 듯"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
